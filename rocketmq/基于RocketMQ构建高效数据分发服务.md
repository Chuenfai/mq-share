本次分享将按照以下几个方面讲解：

1. 场景分析及技术选型
2. 基于RocketMQ构建企业级消息分发服务



#### 场景介绍

单位的项目基本上是面向互联网数据的分析等领域，数据种类比较多，主要包括短文本（微博等）、长文本（网页文章、公众号文章等）、非格式化数据（图片、音频、小视频）。首先，这些数据是流式的，即从业务角度来看具有很强的时效性，需要得到快速的处理；其次，这些数据在尺寸上比较复杂，有短文本、日志这样的小消息（不大于500B），也要文章这样的中等消息（1k-2k），还有图片等这样的大消息（1M-5M），所以我们的系统基于RocketMQ构建了一系列的Pipeline来做数据的实时分发和处理。

![处理流程.png](https://raw.githubusercontent.com/Chuenfai/mq-share/master/img/1.png)

#### 技术选型

在初期选型时，市面上可供选择的可靠的MQ产品并不是很多，当时市面上有RabbitMQ、Kafka、RocketMQ、ZeroMQ、Redis等。如下，当时通过对比了各自的优缺点。因为稳定性和可控性是我们最看重的，所以最后剩下Kafka和RocketMQ两种选择，由于部署环境特殊，必须保证任何情况下的服务可用，另外RocketMQ相比于Kafka在运维层面更简单（不需要引入ZK），所以我们最终选择了RocketMQ来构建消息分发服务。

|          | 优点                                                         | 缺点                                                         |
| -------- | ------------------------------------------------------------ | :----------------------------------------------------------- |
| RabbitMQ | 1、相对成熟<br/>2、支持多种协议（AMQP、MQTT、SMTP）<br/>3、企业级消息中间件，在可靠性、可用性、扩展性、功能丰富等方面表现卓越 | 1、开发语言是Erlang，组内几乎没有熟悉的同学<br/>2、重量级MQ，吞吐量一般 |
| Kafka    | 1、读写性能很高<br/>2、高吞吐量<br/>3、比较成熟<br/>4、在Broker进程被Kill的时，能在保证吞吐量的情况下，不丢消息，可靠性比较高 | 1、复杂性，需要zk的支持<br/>2、宿主机掉电场景下，Kafka吞吐量急剧下降，几乎不可用 |
| RocketMQ | 1、读写性能很高<br/>2、高吞吐量<br/>3、底层构建语言为Java，方便进行二次开发<br/>4、在Broker进程被Kill的场景，能在保证吞吐量的情况下，不丢消息，可靠性都比较高 | 1、生态一般，还在发展中<br/>                                 |
| ZeroMQ   | 1、读写性能极高<br/>2、适用于大吞吐量的场景                  | 1、API比较底层<br/>2、仅提供非持久化消息存储<br/>3、适合在上面构建新的服务，而不是作为一个独立的组件来使用<br/> |
| Redis    | 1、内存数据库，性能比较高<br/>2、社区比较活跃                | 1、不是专业的消息队列，很多关于消息的功能不够完善，只能当做一个轻量级的队列服务来使用<br/>2、应对大消息的场景时写性能下降的厉害<br/> |

#### 最佳实践

我们的消息服务经历过几次调整，目前的架构算是比较稳定，已在线上稳定运行了一年多时间。如图所示，我们目前基于RocketMQ以及不同的业务场景搭建了多套独立的Broker主从集群。

![MQ服务架构](https://raw.githubusercontent.com/Chuenfai/mq-share/master/img/2.png)

##### 多客户端

在业务中开始使用RocketMQ时，由于RocketMQ当时只有Java版本和C++版本的客户端API，所以我们面临的第一个挑战是如何基于常用语言发布、订阅消息，在调研了很多技术框架的多客户实现方式后，我们决定基于Thrift来解决跨语言的问题，当然接口实现上非常简单，只有send和get（映射到RocketMQ的pull方式）两个方法，send方法负责生产消息，支持tag，get方法负责消费消息。这样也同时降低了业务层开发的复杂度。

##### 自动化部署

在开始构建消息服务时，Broker集群是由运维人员手动进行部署的，由于这个过程会引入人为因素导致的问题，为了解决服务部署的繁琐性问题，我们基于Ansible构建了一系列自动化部署的playbook，大大降低了人为原因带来的问题以及配置的繁琐性，现在只要在基本环境上启动自动化部署脚本即可完成集群的部署，还可以实现Broker的快速扩容。

##### 服务运维

RocketMQ本身自带了很多运维命令以及WebUI管理界面，但是考虑到操作的便捷性，我们将RocketMQ的监控与其他服务服务的运维管理进行的合并，即统一使用Zabbix来实现消息集群的运维管理。运维管理服务是基于Zabbix进行了二次开发，对消息服务的运维管理分成服务级别和业务级别两个层次。

* 服务级

服务级别包括集群状态、Broker上下线、Broker读写权限控制、Namesrv节点状态等监控、各节点磁盘使用率情况等。通过分析broker.log和storeerror.log文件，可以快速发现Broker的当前的一些运行情况，当出现严重的运行异常时，及时通知开发者进行BUG修复。

* 业务级

业务级别包括的内容比较多，除了集成了常用的mqadmin命令（创建topic、创建队列、按照id查询消息、按照Tag查询消息、按照Key查询消息、查询生产组和消费组的生产消费状态、查询Broker和消费组的堆积情况等）之外，还扩展一些其他的监控项，比如查询某天中0点到给定时间点某个Topic的生产消息量以及某个消费组的消费总量、某个Topic的在不同时间粒度下生产增量曲线等（可以通过分析stats.log日志来实现）。

##### 版本升级

版本升级是一个常见的问题，我们的场景要求在不影响上层服务的基础上对Broker集群进行热升级。 单纯的shutdown可能出现服务不可用的风险，我们的做法是同时构建一套新版本的集群，然后更新Namesrv，然后将老版本的集群禁写，最后当老集群上的所有数据都过期后停止即可。

![image](https://raw.githubusercontent.com/Chuenfai/mq-share/master/img/3.png)

在升级过程中需要注意几点：

* 两套集群的Namesrv是相互独立的，即老版本的Broker不会注册到新集群的Namesrv上，而新版本的Broker也不会注册到老版本的Namesrv上。
* 通过配置一个Namesrv的域名服务来让业务客户端动态感知Namesrv地址的变化，进而能够动态地更新Topic的路由。
* 当重新配置完Namesrv的路由后，需要等待一段时间再去对老集群进行禁写操作（客户端默认是两分钟拉取一次Namesrv地址）。
* 新版本的集群规模尽量与老版本保持一致。

##### 踩过的坑

* 调整MQ的内存使用情况

之前线上Broker集群在突发流量情况下出现过某几个节点内存使用率爆表的情况，导致机器异常卡顿，经跟踪发现是系统调优时并没有限制MQ的内存使用情况（os.sh注释掉了），最后的解决办法是调整系统参数（vm.min_free_kbytes）限制MQ的内存使用。

![image](https://raw.githubusercontent.com/Chuenfai/mq-share/master/img/4.png)

这个系统参数是内核最小free的内存阈值，用于计算内存水位线，使得各低端内存区域按照一定比例，预留内存。预留内存可以避免内核在紧急情况下分配不出内存而导致的死锁问题；当然也不能调的过高，否则容易导致系统判定内存不足抛出OOM异常。

* 客户端频繁触发流控

之前在将Broker集群由3.5版本升级到4.2版本时，发现客户端在大流量时间节点下频繁的触发流控（业务客户端对写入速率没有做控制）。

![image](https://raw.githubusercontent.com/Chuenfai/mq-share/master/img/5.png)

这个是RocketMQ后来新引入的特性，属于一种服务端保护机制，对MQ进行大流量压力测试时容易出现，解决方案有两种：

​	a) 集群扩容，即对Broker集群增加流量分摊并行度，从而降低每个节点的负载

​	b) 放宽限制，调大osPageCacheBusyTimeOutMills、waitTimeMillsInSendQueue以及sendThreadPoolQueueCapacity参数

#### 近期做的事

* 服务容器化，为了更好的对现有的系统进行运维管理，我们目前正进行系统的微服务化改造，底层基于K8s和Docker来构建基础的运行时环境，消息服务作为系统中很重要的一个环节，也需要纳入到改造的范畴中来，基于Docker和RocketMQ来构建高性能、高可靠的消息分发服务还在测试中，后续如果有比较好的使用经验会继续跟大家分享交流。

